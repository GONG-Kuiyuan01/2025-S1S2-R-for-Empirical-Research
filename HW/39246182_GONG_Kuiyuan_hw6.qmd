---
title: "Homework 6"
format: pdf
author: "GONG Kuiyuan"
---

Student ID: 39-246182

Student name: GONG Kuiyuan

Preferred name: Eddie

## Answers:

**Task 1**: I first set the working dictionary and use the function `read.csv` to access and store the data.

```{r}
#setwd("~/Desktop/R for empirical research/HW/")
netflix <- read.csv("netflix_titles.csv")
```

**Task 2**: I use the verb `select()` together with `contains()` to select the columns I need. Here I selected columns that contain *at*. I also demonstrated the other approach where we can use `where(is.character)` to filter those empty columns. I need to use `head()` to restrict the list shown, otherwise it can't not be rendered.

```{r}
library(pacman)
p_load(tidyverse)
netflix |>
  select(contains("at")) |>
  head(5)
#netflix |> select(where(is.character))
```

**Task 3**: I choose to only print the new variables, otherwise the file can't be rendered due to the big tibble.

```{r}
netflix_2 <- netflix |>
  mutate(
    length = if_else(type == "Movie", parse_number(duration), NA),
    seasons = if_else(type == "TV Show", parse_number(duration), NA)
  ) |>
  relocate(length, seasons, .before = 1)

netflix_2 |>
  select(length, seasons) |>
  head() |>
  print()
#print(netflix_2)
```

**Task 4**: Yes, we can observe from the data that there are more TV shows and the number of seasons over the time. A large number of observations are clustered between the years 2000 and 2025. Hypothesis: the number of TV shows and their seasons proliferates as technologies advance over the time.

```{r}
ggplot(
  data = netflix_2,
  mapping = aes(x = release_year, y = seasons)
) + 
  geom_point()
```

**Task 5**: Yes, we can observe a similar trend with the above plot. The number of movies increase over the time, however, the length of movies is commonly from 0 to 150 minutes, mostly concentrate around 100 minutes. Hypothesis: the number of movies proliferate as technologies advance over the time, however, the length of movies doesn't change a lot but stabilize on a scale.

```{r}
ggplot(
  data = netflix_2,
  mapping = aes(x = release_year, y = length)
) + 
  geom_point()
```

**Task 6**: The data is not "tidy" because food, clothing and other categories are column names here. The "tidy" data should be values of a single “category” column, with corresponding amounts in another column. For example, in the column "category" there will be values like "food" and "clothing".

```{r}
library(pacman)
p_load(griffen, griffendata)
print(expenditure_data1)
```

**Task 7**: 
```{r}
expenditure_data1 |>
  mutate(total_expenditure = food + clothing + housing + alcohol) |>
  print()
```

**Task 8**:
```{r}
expenditure_pivot <- expenditure_data1 |>
  pivot_longer(
    cols = contains(c("food", "clothing", "housing", "alcohol")), 
    names_to = 'category', 
    values_to = 'expenditure') |>
  print()
```

**Task 9**:

```{r}
expenditure_pivot <- expenditure_pivot |>
  group_by(id) |>
  summarise(total_expenditrue = sum(expenditure)) |>
  print()
```

**Task 10**: I prefer to use `group_by()` and `summarise()` to get the target value because it only prints out the total value and mutate other information, which is clearer for me. 

**Task 11**:

```{r}
library(pacman)
p_load(griffen, griffendata)
print(expenditure_data2)
```

**Task 12**: There are many ways to know the number of columns of a tibble. For example, we can know the size of the tibble when we print it out, or, we can use `ncol()` to report the number for us.

```{r}
ncol(expenditure_data2)
```

**Task 13**: Without pivoting it longer, we need to manually add every term inside of the function in order to get the total expenditures.

```{r}
#expenditure_data2 |>
  #mutate(
  #total_expenditure = item1 + item2 + item3 + item4...+ item200) |>
  #print()
```

**Task 14**:

```{r}
expenditure_data2 |>
  pivot_longer(
    cols = contains("item"), 
    names_to = 'category', 
    values_to = 'expenditure') |>
  group_by(id) |>
  summarise(total_expenditrue = sum(expenditure)) |>
  print()
```

**Task 15**: Because the data now is "tidy" in the sense that we can manipulate the data easily using `group_by(id)` and continue to do other operations according to our needs. It transforms multiple expenditure categories (like food...) from separate columns into rows under a single column.

**Task 16**:
```{r}
#| error: true
full_cps |>
  filter(state == "Kentacky") |> 
  group_by(year, education_category, race) |>
  summarise(
    n = n(),
    wage = mean(wage, na.rm = TRUE)) |>
  filter(n > 10)
```

**Task 17**: First, there is not such variable named "race" in the data, which makes `group_by()` invalid. Second, there is a misspelling of "Kentacky" instead of "Kentucky", which makes the `filter()` invalid. Third, the previous writing style is very difficult for other people to read and debug the code, it may potentially increase the probability of making mistakes. 
```{r}
names(full_cps)

full_cps |> 
  filter(state == "Kentacky")

full_cps |> 
  filter(state == "Kentucky")
```